# -*- coding: utf-8 -*-
"""ML Filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kBqZP6rYpHOUo3c6R3a7U7n0bKNESA7X
"""



import nltk
#nltk.download('punkt', force=True)
from nltk.tokenize import word_tokenize
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('punkt')
#import moviepy.editor as mp
import os
import glob
#import speech_recognition as sr
import csv
import numpy as np
import pandas as pd
from qdrant_client import QdrantClient
from qdrant_client.http import models
from transformers import AutoModel, AutoTokenizer
import torch

gsheetkey = "1Fp1T2Q1K-DVElVS-ta_QgqKj27y5wP88oC5p1O9gypA"

#sheet name
sheet_name = 'Data Baru'

url=f'https://docs.google.com/spreadsheet/ccc?key={gsheetkey}&output=xlsx'
df = pd.read_excel(url,sheet_name=sheet_name)
df.head()

gsheetkey = "1cRSp9v-K2pyx_ArVDGM4yaZCgjfofXkIhnA4aDjzYr4"

#sheet name
sheet_name = 'DATA ONBOARDING'

url=f'https://docs.google.com/spreadsheet/ccc?key={gsheetkey}&output=xlsx'
df2 = pd.read_excel(url,sheet_name=sheet_name)
df2.head()

client = QdrantClient(":memory:")

my_collection = "text_collection"
client.recreate_collection(
    collection_name=my_collection,
    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE)
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModel.from_pretrained('gpt2')#.to(device) # switch this for GPU

def get_embeddings(row):
    tokenizer = AutoTokenizer.from_pretrained('gpt2')
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    # Menggabungkan beberapa kolom teks menjadi satu teks
    combined_text = ' '.join(row[['rescue_story_1', 'rescue_story_2', 'rescue_story_3', 'request_story']].astype(str).values)

    # Tokenisasi teks gabungan
    inputs = tokenizer(combined_text, padding=True, truncation=True, max_length=128, return_tensors="pt")

    # Matikan perhitungan gradien untuk operasi selanjutnya.
    with torch.no_grad():
        outputs = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()

    # Mengembalikan hasil embeddings yang dihitung.
    return outputs

df2['embeddings'] = df2.apply(get_embeddings, axis=1)
np.save("vectors", np.array(df2['embeddings']))

df2.head()

payload = df2[['group', 'age', 'sterilisasi', 'embeddings']].to_dict(orient="records")

# Set the expected size for the vector embeddings
expected_vector_size = 768

# Check if the tokenizer has a padding token, and if not, set it to the eos_token
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})

# Define a function for mean pooling of token embeddings
def mean_pooling(model_output, attention_mask):
    # Extract token embeddings from the model output
    token_embeddings = model_output[0]

    # Expand the attention mask to match the size of token embeddings
    input_mask_expanded = (attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float())

    # Calculate the sum of token embeddings, considering the attention mask
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)

    # Calculate the sum of the attention mask (clamped to avoid division by zero)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    # Return the mean-pooled embeddings
    return sum_embeddings / sum_mask

# Initialize a list to store text embeddings
text_embeddings = []

# Loop through each transcript in the 'Rescue Story' column of the 'df' variable
for transcript in df2['request_story']:
    # Tokenize the transcript, ensuring padding and truncation, and return PyTorch tensors
    inputs = tokenizer(transcript, padding=True, truncation=True, max_length=128, return_tensors="pt")

    # Perform inference using the model with the tokenized inputs
    with torch.no_grad():
        embs = model(**inputs)

    # Calculate mean-pooled embeddings using the defined function
    embedding = mean_pooling(embs, inputs["attention_mask"])

    # Ensure the embeddings are of the correct size by trimming or padding
    embedding = embedding[:, :expected_vector_size]

    # Append the resulting embedding to the list
    text_embeddings.append(embedding)

ids = list(range(len(df2)))

# Convert PyTorch tensors to lists of floats
text_embeddings_list = [[float(num) for num in emb.numpy().flatten().tolist()[:expected_vector_size]] for emb in text_embeddings]

client.upsert(collection_name=my_collection,
              points=models.Batch(
                  ids=ids,
                  vectors=text_embeddings_list,
                  payloads=payload
                  )
              )

from textblob import TextBlob

def calculate_sentiment_score(text):
    # Create a TextBlob object
    blob = TextBlob(text)

    # Get the sentiment polarity (-1 to 1, where -1 is negative, 0 is neutral, and 1 is positive)
    sentiment_score = blob.sentiment.polarity

    return sentiment_score


# Example usage:
text_example = df2['request_story'].iloc[len(df2) - 1]
sentiment_score_example = calculate_sentiment_score(text_example)
print(f"Sentiment Score: {sentiment_score_example}")

df2['Sentiment Score'] = df2['request_story'].apply(calculate_sentiment_score)
df2.head()

df2['avg_embeddings'] = df2['embeddings'].apply(lambda x: np.mean(x, axis=0))
df2['Opinion_Score'] = 0.7 * df2['avg_embeddings'] + 0.3 * df2['Sentiment Score']

df2.head()

def get_recommendations(anjing):
    # Find the row corresponding to the given movie name
    query_row = df2[df2['request_story'] == anjing]  # Fix: use 'movie_name' instead of 'Name'

    if not query_row.empty:
        # Convert the 'Opinion_Score' column to a NumPy array
        opinion_scores_array = np.array(df2['Opinion_Score'].tolist())
        # Upsert the 'Opinion_Score' vectors to the Qdrant collection
        opinion_scores_ids = list(range(len(df2)))
        # Convert the 'Opinion_Score' array to a list of lists
        opinion_scores_list = opinion_scores_array.reshape(-1, 1).tolist()

        client.upsert(
            collection_name=my_collection,
            points=models.Batch(
                ids=opinion_scores_ids,
                vectors=opinion_scores_list
            )
        )
        # Define a query vector based on the opinion score you want to find similar movies for
        query_opinion_score = np.array([0.8] * 768)  # Adjust as needed

        # Perform a similarity search
        search_results = client.search(
            collection_name=my_collection,
            query_vector=query_opinion_score.tolist(),
            limit=10
        )

        # Extract movie recommendations from search results
        recommended_anjing_ids = [result.id for result in search_results]
        recommended_anjing = df2.loc[df2.index.isin(recommended_anjing_ids)]

        # Display recommended movies
        print("Recommended Result:")
        print(recommended_anjing[['request_story', 'rescue_story_1', 'rescue_story_3', 'rescue_story_3']])  # Fix: use 'recommended_movies' instead of 'recommended_pet'
    else:
        print(f"Anjing '{anjing}' not found in the dataset.")

# Example usage:

from tensorflow.keras.models import save_model

if __name__ == '__main__':
    # DO NOT CHANGE THIS CODE
    model = get_recommendations()
    save_model(model, "get_recommendation.h5")


"""logikanya nanti hasil yang disini itu buat nyari si anjing . backend nya nyari anjing dari 3 rescue story"""